One of the most challenging area of Machine Learning is the one that regards the language and it is known as Natural Language Processing (NLP). It is true that all the area of Machine Learning can be complex and challenging at some level, but NLP is particularly difficult as it requires to explore human communication and, somehow, human consciousness. Moreover, while it is relatively easy to encode an image in terms of data (i.e. a bidimensional matrix), or a physics experiment (that is basically a .csv file), it is extremely harder to encode a text as a number or a vector.

But what do we actually want to solve? What are the so difficult tasks that I’m talking about? Well, for example, in this blog I will discuss an example of text classification. In particular, we want to classify wether or not the news are fake.

Moreover, we want to face this task using the State of Art methods proposed by BERT and a special encoder released by Google known as Universal Sentence Encoder. Plus, we will use a traditional Machine Learning tool that is becoming more and more popular for its easiness of use and its interesting features: PyCaret.

The theory that is behind BERT or the Universal Sentence Encoder is deep and complex, and it would be necessary more than a single blog to explain it further. Moreover, I would still not be able to explain them as precisely as their creators, so I’m not even going to try it.

On the other hand, the practical usage of these tools are really simple and will properly be explained during this post.
